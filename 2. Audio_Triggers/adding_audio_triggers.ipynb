{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO+IrEdrlpk/Lf2DE6lD+B6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Script for adding Audio Channel Triggers to Auditory Stimuli**\n","\n","## Script Info\n","---\n","- This script takes a mono channel audio file and creates a new stereo .wav file where the second audio channel contains the trigger/event information for segmenting neuroscientific data.\n","\n","- Using audio channel triggers is useful for minimizing latencies between stimulus onset and trigger onset. This is due to the fact that both stimulus and trigger are included in the same file - meaning that every buffering delay that occurs applies to stimulus and trigger with the exact same latency.\n","\n","- One drawback of using audio channel triggers with stereo files is that all triggers are sent with the same portcode, independent of experimental condition. This can be fixed by either using varying pulse durationsm varying pulse frequency (e.g.: bursts of multiple pulses) or by creating files with more than two channels.  \n","\n","- The aforementioned issue can further be tackled by creating one-dimensional array of manually chosen portcodes where each portcode's position in the array matches with the trial condition's position in the soft-trigger-based event array.\n","- => For example: Our original soft-trigger event array is [260, 260, 260, 260, 260], where the 3rd and 4th trial were target trials, while all the other trials in the array were distractors. Accordingly, we need to distinguish these two conditions for our statistical analysis. As such, we replace the aforementioned event array with our manually created array [1, 1, 2, 2, 1]. Now we can extract re-segment our data based on these new portcodes and can extract target and distractor trials seperately.\n","\n","- This script's procedure is used in multiple studies of the University of Zurich's research group 'Computational Neuroscience of Speech & Hearing'.\n","\n","- You can easily adapt this script to your own needs by swapping out the audio files and adapting parameters like trigger pulse width, channel position, etc.\n","\n","- The script was written and tested in a Google Colab notebook. You can easily just drag & drop this script in there and have it run. Make sure to plug in your own directory paths and already audio-triggered syllable .wav  file. Alternatively, you can use the cut and triggered /da/ syllable file, that's included in this repository.\n","\n","- For questions and inquiries, contact julian.ockelmann@uzh.ch\n","\n","## Contact\n","---\n","Author of this script:\n","- Julian Ockelmann, PhD\n","- Computational Neuroscience of Speech & Hearing\n","- University of Zurich\n","- julian.ockelmann@uzh.ch"],"metadata":{"id":"2xdcRyxG85qc"}},{"cell_type":"code","source":["# Install necessary libraries (if necessary)\n","!pip install pydub\n","!pip install numpy"],"metadata":{"id":"0LBBXhLN9xwb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705671885311,"user_tz":-60,"elapsed":27704,"user":{"displayName":"Julian Ockelmann","userId":"12030700450975087399"}},"outputId":"1dc5524d-5d22-46d4-e561-ace8e69a01cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pydub\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Installing collected packages: pydub\n","Successfully installed pydub-0.25.1\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n"]}]},{"cell_type":"code","source":["# Import necessary modules\n","import os\n","import numpy as np\n","from pydub import AudioSegment\n","from scipy.io import wavfile\n","from google.colab import files                                                  # Only necessary if you're executing the code in a google colab environment"],"metadata":{"id":"-MyxOJkq-Ixp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Optional: Mount Google Drive for using cloud-stored data\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mvIKn-129aSF","executionInfo":{"status":"ok","timestamp":1705676052213,"user_tz":-60,"elapsed":1555,"user":{"displayName":"Julian Ockelmann","userId":"12030700450975087399"}},"outputId":"68150d99-7b54-404d-cbb9-e92bc46b3c73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Set Directories (Replace with desired Working Directory)\n","wd = '/content/drive/MyDrive/Colab Notebooks/Utility/Audio_Files/'                      # Folder containing untriggered stimulus files\n","os.chdir(wd)"],"metadata":{"id":"yYsh9vs-69uH","executionInfo":{"status":"ok","timestamp":1705677323672,"user_tz":-60,"elapsed":188,"user":{"displayName":"Julian Ockelmann","userId":"12030700450975087399"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ml0UCIvy1j_-","executionInfo":{"status":"ok","timestamp":1705677571376,"user_tz":-60,"elapsed":885,"user":{"displayName":"Julian Ockelmann","userId":"12030700450975087399"}},"outputId":"829b2035-20c6-465d-c29e-c693cb28ffac"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-40-48872ede7b67>:5: WavFileWarning: Chunk (non-data) not understood, skipping it.\n","  sample_rate, speech_train = wavfile.read(f'Audio_File_{i}.wav')             # Read your wav (mono audio).\n"]}],"source":["# Set Width of Trigger Pulse\n","pulse_width = 1000                                                              # In this case, 1000 means 1000 samples rather than milliseconds\n","\n","for i in range(1,6):                                                            # Applies trigger procedure to the numerated audio files 1-5\n","    sample_rate, speech_train = wavfile.read(f'Audio_File_{i}.wav')             # Read your wav (mono audio). # 32-bit or 64-bit float format, otherwise the soundwave is coded as a binary oscillation (idk why)\n","\n","    trigger_train = np.zeros(len(speech_train))                                 # Empty zeros vector.\n","    trigger_train[0:pulse_width] = 1                                            # Set onset to 1 for a certain pulse width.\n","\n","    stereo_train = np.column_stack((speech_train, trigger_train))               # Convert to `stereo_train`.\n","\n","    audio = wavfile.write(\n","      f'Trig_Audio_File_{i}.wav',                                               # Insert desired naming convention where {i} makes sure that your untriggered file 'Audio_1.wav' accordingly becomes e.g.: 'Trig_Audio_1.wav'\n","      sample_rate,\n","      stereo_train\n","      )"]}]}
